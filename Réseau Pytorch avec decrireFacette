import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import scipy.stats as sps
import matplotlib.pyplot as plt

def genDonneesGaussiennes() :
  m, s = (-2, 2), (1, 1)
  al = np.random.randint(0,2)
  return al, sps.norm.rvs(m[al], s[al])


N = 100

X = np.zeros((N))
y = np.zeros((N))

for i in range(N) :
    res, donnee = genDonneesGaussiennes()
    X[i] = donnee
    y[i] = res

X_tensor = torch.tensor(np.transpose(np.atleast_2d(X)), dtype=torch.float32)
y_tensor = torch.tensor(np.transpose(np.atleast_2d(y)), dtype=torch.float32)

class LinearRegressionModel(nn.Module):
    def __init__(self):
        super(LinearRegressionModel, self).__init__()
        self.linearReluStack = nn.Sequential(
                                        nn.Linear(1,3),
                                        nn.ReLU(),
                                        nn.Linear(3,3),
                                        nn.ReLU(),
                                        nn.Linear(3, 1),
                                        )

    def forward(self, x):
      logits = self.linearReluStack(x)
      return logits

    def decrireFacette0(self, x, layer_index):
      current_input = x
      for i, layer in enumerate(self.linearReluStack):
        current_input = layer(current_input)
        if i == layer_index:
          activation = current_input.clone().detach().numpy()
          activation_binaires = (current_input > 0).int().detach().numpy()
          print(f"\nActivations for Layer {i} : {activation}")
          print(f"Active neurons: {(activation > 0).astype(int)}")
          return activation
      return None

#retourne l'activation d'une seule couche du réseau
    def decrireFacette(self, x, layer_index):
      current_input = x
      for i, layer in enumerate(self.linearReluStack):
        current_input = layer(current_input)
        if i == layer_index:
          activation_binaires = (current_input > 0).int().detach().numpy()
          return activation_binaires
      return None

#retourne une liste de vecteurs des activations de toutes les couches
    def activations(self, x):
      activation = [x.detach().numpy()]
      for layer_index in range(0, len(self.linearReluStack), 2):
        activation_couche = self.decrireFacette(x, layer_index)
        activation.append(activation_couche)
      return activation


model = LinearRegressionModel()

criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

print(model.to())

num_epochs = 1000
listePerte = np.zeros(num_epochs)
for epoch in range(num_epochs):
    outputs = model(X_tensor)
    loss = criterion(outputs, y_tensor)
    listePerte[epoch] = loss.detach().numpy().mean()
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

plt.plot(range(1000), np.array(listePerte))
plt.show()


n = 20
X = np.zeros((n))
y = np.zeros((n))

for i in range(n) :
    res, donnee = genDonneesGaussiennes()
    X[i] = donnee
    y[i] = res

X_tensor = torch.tensor(np.transpose(np.atleast_2d(X)), dtype=torch.float32)
y_tensor = torch.tensor(np.transpose(np.atleast_2d(y)), dtype=torch.float32)
predicted = model(X_tensor).detach().numpy()
for i in range(n) :
    print(X_tensor[i], y_tensor[i], predicted[i])

X = np.linspace(-4, 4, 1000)
X_tensor = torch.tensor(np.transpose(np.atleast_2d(X)), dtype=torch.float32)
predicted = model(X_tensor).detach().numpy()
plt.plot(X, predicted)
plt.show()

#test de decrireFacette et activations
x_test = torch.tensor([[0.192]], dtype=torch.float32)
layer_index = 2
print("Activation de la couche ", layer_index, model.decrireFacette(x_test, layer_index))
print("Activations du réseau ", model.activations(x_test))
